{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7615aa",
   "metadata": {
    "id": "eb7615aa"
   },
   "source": [
    "### Interpretability of Attention-based MIL\n",
    "\n",
    "In this session, we will explore two post-hoc interpretability methods for understanding the behaviour of an Attention-based MIL model trained to subtype lung cancer into LUAD and LUSC, the two most common lung cancer subtypes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08e37d",
   "metadata": {
    "id": "cf08e37d"
   },
   "source": [
    "#### On the importance of interpretability in medical AI applications\n",
    "\n",
    "- **Trust & control**: Medical AI systems are often used in critical decision-making processes that directly impact patients' health and lives. Interpretability aims to provide the right level of insights and control over the system such that trust and confidence can be established between users and the system.\n",
    "\n",
    "\n",
    "- **Insights and Understanding**: Interpretable models are also valuable to provide insights into the underlying factors or features that contribute to a prediction. In medical AI, this understanding can be critical for healthcare professionals to gain insights into disease mechanisms, identify risk factors, or discover novel biomarkers. This aspect goes in the direction of biomarker discovery, where we assume that the AI system might use different features than what form the current standard (e.g., the current grading criteria in cancer).\n",
    "\n",
    "\n",
    "- **Error Analysis and Diagnosis**: Interpretability helps in error analysis, allowing the identification and understanding of the model's mistakes or mispredictions. In medical AI, where misdiagnoses can have severe consequences, interpretability enables clinicians to evaluate cases where the model failed and diagnose potential pitfalls or limitations. This feedback loop can guide improvements in the model, dataset, or feature engineering, leading to better performance and more reliable predictions.\n",
    "\n",
    "\n",
    "And the wishful thinking considerations...\n",
    "\n",
    "\n",
    "- **Legal and Ethical Considerations**: The interpretability of AI models could be used for addressing legal and ethical concerns. In healthcare, decisions made by AI systems need to be explainable to patients, healthcare professionals, regulatory bodies, and other stakeholders. By providing interpretability, AI systems can adhere to legal requirements, such as the General Data Protection Regulation (GDPR), which grants individuals the right to an explanation for automated decisions that significantly impact them.\n",
    "\n",
    "\n",
    "- **Safety and Robustness**: Deep learning models are susceptible to biases, adversarial attacks, or data distribution shifts that can lead to incorrect or unreliable predictions. Interpretability helps in detecting these issues and assessing the model's safety and robustness. By understanding the model's internal workings, it becomes possible to identify potential biases, investigate cases where the model may be overconfident or underperform, and design safeguards to mitigate risks.\n",
    "\n",
    "\n",
    "- **Regulatory Compliance**: Interpretability is increasingly becoming a regulatory requirement in various domains, including healthcare. Regulatory bodies, such as the U.S. Food and Drug Administration (FDA), often demand explanations and justifications for the decisions made by AI systems before approving their deployment. Interpretability allows the model's behavior to be audited, validated, and aligned with regulatory standards, ensuring compliance and patient safety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QcNSlWhRR-X3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcNSlWhRR-X3",
    "outputId": "1463bbdc-5441-4500-85d1-a51bedcaab3e"
   },
   "outputs": [],
   "source": [
    "# All data can be downloaded here: https://drive.google.com/drive/folders/1TmAfG7EWC1hjD7cHFGiJzUx2y3jLXdcP?usp=sharing\n",
    "# Once downloaded, transfer the data into your local cloned repo \n",
    "\n",
    "use_drive = False\n",
    "if use_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !mkdir -p \"/content/drive/My Drive/ai4healthsummerschool/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b4e6950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4e6950",
    "outputId": "d9d2c01a-50af-42c3-c13f-a5985d155817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABMIL(\n",
       "  (inst_level_fc): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_attn): AttentionTanhSigmoidGating(\n",
       "    (tanhV): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (sigmU): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (w): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (bag_level_classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and print ABMIL model previously trained\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "class AttentionTanhSigmoidGating(nn.Module):\n",
    "    def __init__(self, D=64, L=64, dropout=0.25):\n",
    "        r\"\"\"\n",
    "        Global attention pooling layer with tanh non-linearity and sigmoid gating (Ilse et al. 2018).\n",
    "\n",
    "        Args:\n",
    "            D (int): input feature dimension.\n",
    "            L (int): hidden layer dimension. Notation changed from M from Ilse et al 2018, as M is overloaded to also describe # of patch embeddings in a WSI.\n",
    "            dropout (float): Dropout probability.\n",
    "\n",
    "        Returns:\n",
    "            A_norm (torch.Tensor): [M x 1]-dim tensor of normalized attention scores (sum to 1)\n",
    "        \"\"\"\n",
    "        super(AttentionTanhSigmoidGating, self).__init__()\n",
    "        self.tanhV = nn.Sequential(*[nn.Linear(D, L), nn.Tanh(), nn.Dropout(dropout)])\n",
    "        self.sigmU = nn.Sequential(*[nn.Linear(D, L), nn.Sigmoid(), nn.Dropout(dropout)])\n",
    "        self.w = nn.Linear(L, 1)\n",
    "\n",
    "    def forward(self, H, return_raw_attention=False):\n",
    "        A_raw = self.w(self.tanhV(H).mul(self.sigmU(H))) # exponent term\n",
    "        A_norm = F.softmax(A_raw, dim=0)                 # apply softmax to normalize weights to 1\n",
    "        assert abs(A_norm.sum() - 1) < 1e-3              # Assert statement to check sum(A) ~= 1\n",
    "        if return_raw_attention:\n",
    "            return A_norm, A_raw\n",
    "        return A_norm\n",
    "\n",
    "\n",
    "class ABMIL(nn.Module):\n",
    "    def __init__(self, input_dim=320, hidden_dim=64, dropout=0.25, n_classes=2):\n",
    "        r\"\"\"\n",
    "        Attention-Based Multiple Instance Learning (Ilse et al. 2018).\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): input feature dimension.\n",
    "            hidden_dim (int): hidden layer dimension.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        super(ABMIL, self).__init__()\n",
    "        self.inst_level_fc = nn.Sequential(*[nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]) # Fully-Connected Layer, applied \"instance-wise\" to each embedding\n",
    "        self.global_attn = AttentionTanhSigmoidGating(L=hidden_dim, D=hidden_dim)                              # Attention Function\n",
    "        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)                                            # Bag-Level Classifier\n",
    "\n",
    "    def forward(self, X: torch.randn(100, 320), return_raw_attention=False):\n",
    "        r\"\"\"\n",
    "        Takes as input a [M x D]-dim bag of patch features (representing a WSI), and outputs: 1) logits for classification, 2) un-normalized attention scores.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): [M x D]-dim bag of patch features (representing a WSI)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [1 x n_classes]-dim tensor of un-normalized logits for classification task.\n",
    "            A_norm (torch.Tensor): [M,]- or [M x 1]-dim tensor of attention scores.\n",
    "        \"\"\"\n",
    "        H_inst = self.inst_level_fc(X)         # 1. Process each feature embedding to be of size \"hidden-dim\"\n",
    "        if return_raw_attention:\n",
    "            A_norm, A_raw = self.global_attn(H_inst, return_raw_attention=True)\n",
    "        else:\n",
    "            A_norm = self.global_attn(H_inst)      # 2. Get normalized attention scores for each embedding (s.t. sum(A_norm) ~= 1)\n",
    "        z = torch.sum(A_norm * H_inst, dim=0)  # 3. Output of global attention pooling over the bag\n",
    "        logits = self.bag_level_classifier(z).unsqueeze(dim=0)   # 4. Get un-normalized logits for classification task\n",
    "        try:\n",
    "            assert logits.shape == (1,2)\n",
    "        except:\n",
    "            print(f\"Logit tensor shape is not formatted correctly. Should output [1 x 2] shape, but got {logits.shape} shape\")\n",
    "        \n",
    "        if return_raw_attention:\n",
    "            return logits, A_raw\n",
    "        return logits, A_norm\n",
    "\n",
    "    def captum(self, X: torch.randn(100, 320)):\n",
    "        r\"\"\"\n",
    "        Takes as input a [M x D]-dim bag of patch features (representing a WSI), and outputs: 1) logits for classification, 2) un-normalized attention scores.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): [M x D]-dim bag of patch features (representing a WSI)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [1 x n_classes]-dim tensor of un-normalized logits for classification task.\n",
    "            A_norm (torch.Tensor): [M,]- or [M x 1]-dim tensor of attention scores.\n",
    "        \"\"\"\n",
    "        H_inst = self.inst_level_fc(X)         # 1. Process each feature embedding to be of size \"hidden-dim\"\n",
    "        A_norm = self.global_attn(H_inst)      # 2. Get normalized attention scores for each embedding (s.t. sum(A_norm) ~= 1)\n",
    "        z = torch.sum(A_norm * H_inst, dim=0)  # 3. Output of global attention pooling over the bag\n",
    "        logits = self.bag_level_classifier(z).unsqueeze(dim=0)   # 4. Get un-normalized logits for classification task\n",
    "        try:\n",
    "            assert logits.shape == (1,2)\n",
    "        except:\n",
    "            print(f\"Logit tensor shape is not formatted correctly. Should output [1 x 2] shape, but got {logits.shape} shape\")\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = ABMIL(input_dim=320, hidden_dim=64).to(device)\n",
    "\n",
    "if use_drive:\n",
    "    path = '/content/drive/My Drive/ai4healthsummerschool/abmil.ckpt'\n",
    "else:\n",
    "    path = 'abmil.ckpt'\n",
    "    \n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c25de",
   "metadata": {
    "id": "472c25de"
   },
   "source": [
    "#### Interpreting Attention-based MIL with Attention weights\n",
    "\n",
    "Attention weights are a mechanism used in deep learning models to determine the importance or relevance of different parts of the input data. These weights can be utilized to interpret deep learning predictions by providing insights into which parts of the input contribute more strongly to the model's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c0ccf",
   "metadata": {
    "id": "640c0ccf"
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3fcb8c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3fcb8c9",
    "outputId": "1a810f85-66ae-459b-a07c-7ee48c43b60e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will analyse the prediction and attention scores of:\n",
      "TCGA-35-3615-01Z-00-DX1.585128eb-6652-4b05-9a83-dc8f242904a6.pt\n",
      "Label:  0\n",
      "Features: torch.Size([3479, 320])\n",
      "tensor([ 1.6617, -0.6829], grad_fn=<SqueezeBackward0>)\n",
      "Shape:  (3479,) Min:  -4.686355113983154 Max:  -1.750939965248108\n"
     ]
    }
   ],
   "source": [
    "# Load features and corresponding label\n",
    "if use_drive:\n",
    "    feats_dirpath='/content/drive/My Drive/ai4healthsummerschool/feats_pt'\n",
    "    csv_fpath='/content/drive/My Drive/ai4healthsummerschool/tcga_lung_splits.csv'\n",
    "else:\n",
    "    feats_dirpath='feats_pt'\n",
    "    csv_fpath='tcga_lung_splits.csv'\n",
    "index = 5  # (LUAD sample)\n",
    "\n",
    "csv = pd.read_csv(csv_fpath)\n",
    "which_labelcol = 'OncoTreeCode_Binarized'\n",
    "csv_split = csv[csv['split']=='test']\n",
    "\n",
    "features = torch.load(os.path.join(feats_dirpath, csv_split.iloc[index]['slide_id']+'.pt'))\n",
    "label = csv_split.iloc[index][which_labelcol]\n",
    "\n",
    "print('We will analyse the prediction and attention scores of:')\n",
    "print(csv_split.iloc[index]['slide_id']+'.pt')\n",
    "print('Label: ', label)\n",
    "print('Features:', features.shape)\n",
    "\n",
    "# Run inference and store attention weights\n",
    "logits, attention = model(features, return_raw_attention=True)\n",
    "logits = logits.squeeze()\n",
    "attention = attention.squeeze().detach().numpy()\n",
    "print(logits)\n",
    "print('Shape: ', attention.shape, 'Min: ', np.min(attention).item(), 'Max: ', np.max(attention).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7x2kFBsnwq4T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "7x2kFBsnwq4T",
    "outputId": "4b5ebc78-b641-4fc1-8a7d-3f217db0a315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openslide-python in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (1.1.2)\r\n",
      "Requirement already satisfied: Pillow in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from openslide-python) (9.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openslide-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7de5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coords: (3479, 2)\n"
     ]
    }
   ],
   "source": [
    "# Visualize the attention weights on the input WSIs\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from openslide import OpenSlide\n",
    "\n",
    "# load coords corresponding to sample of interest\n",
    "if use_drive:\n",
    "    path = '/content/drive/My Drive/ai4healthsummerschool/TCGA-35-3615-01Z-00-DX1.585128eb-6652-4b05-9a83-dc8f242904a6_patches.h5'\n",
    "else:\n",
    "    path = 'data/processed/TCGA-35-3615-01Z-00-DX1.585128eb-6652-4b05-9a83-dc8f242904a6_patches.h5'\n",
    "    \n",
    "with h5py.File(path, \"r\") as f:\n",
    "    coords = f['coords'][:]\n",
    "    print('Coords:', coords.shape)\n",
    "\n",
    "# load whole slide imahe using OpenSlide \n",
    "if use_drive:\n",
    "    slide_path = '/content/drive/My Drive/ai4healthsummerschool/TCGA-35-3615-01Z-00-DX1.585128eb-6652-4b05-9a83-dc8f242904a6.svs'\n",
    "else:\n",
    "    slide_path = 'data/slides/TCGA-35-3615-01Z-00-DX1.585128eb-6652-4b05-9a83-dc8f242904a6.tiff'\n",
    "wsi = OpenSlide(slide_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "R09MoSFKYa85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "R09MoSFKYa85",
    "outputId": "0d079399-0a8e-4fbb-9ebb-94d8e63d12ca"
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(scores, coords, wsi, vis_level=-1,\n",
    "                patch_size=(256, 256),\n",
    "                blank_canvas=False, canvas_color=(220, 20, 50), alpha=0.4,\n",
    "                overlap=0.0, use_holes=True,\n",
    "                convert_to_percentiles=False, thresh=0.5,\n",
    "                max_size=None, custom_downsample = 4,\n",
    "                cmap='coolwarm'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        scores (numpy array of float): Attention scores\n",
    "        coords (numpy array of int, n_patches x 2): Corresponding coordinates (relative to lvl 0)\n",
    "        wsi (openslide): WSI opened with openslide\n",
    "        vis_level (int): WSI pyramid level to visualize\n",
    "        patch_size (tuple of int): Patch dimensions (relative to lvl 0)\n",
    "        blank_canvas (bool): Whether to use a blank canvas to draw the heatmap (vs. using the original slide)\n",
    "        canvas_color (tuple of uint8): Canvas color\n",
    "        alpha (float [0, 1]): blending coefficient for overlaying heatmap onto original slide\n",
    "        blur (bool): apply gaussian blurring\n",
    "        overlap (float [0 1]): percentage of overlap between neighboring patches (only affect radius of blurring)\n",
    "        segment (bool): whether to use tissue segmentation contour (must have already called self.segmentTissue such that\n",
    "                        self.contours_tissue and self.holes_tissue are not None\n",
    "        use_holes (bool): whether to also clip out detected tissue cavities (only in effect when segment == True)\n",
    "        convert_to_percentiles (bool): whether to convert attention scores to percentiles\n",
    "        binarize (bool): only display patches > threshold\n",
    "        threshold (float): binarization threshold\n",
    "        max_size (int): Maximum canvas size (clip if goes over)\n",
    "        custom_downsample (int): additionally downscale the heatmap by specified factor\n",
    "        cmap (str): name of matplotlib colormap to use\n",
    "    \"\"\"\n",
    "\n",
    "    downsample = (0.25, 0.25)\n",
    "    patch_size  = np.ceil(np.array(patch_size)).astype(int)\n",
    "    coords = np.ceil(coords * np.array(downsample)).astype(int)\n",
    "\n",
    "    region_size = wsi.level_dimensions[vis_level]\n",
    "    w, h = region_size\n",
    "\n",
    "    print('\\ncreating heatmap for: ')\n",
    "    print('w: {}, h: {}'.format(w, h))\n",
    "    print('scaled patch size: ', patch_size)\n",
    "\n",
    "    # heatmap overlay: tracks attention score over each pixel of heatmap\n",
    "    # overlay counter: tracks how many times attention score is accumulated over each pixel of heatmap\n",
    "\n",
    "    overlay = np.full(np.flip(region_size), 0).astype(float)\n",
    "    counter = np.full(np.flip(region_size), 0).astype(np.uint16)\n",
    "    count = 0\n",
    "    for idx in range(len(coords)):\n",
    "        score = scores[idx].item()\n",
    "        coord = coords[idx]\n",
    "        # accumulate scores\n",
    "        overlay[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] += score\n",
    "        # accumulate counter\n",
    "        counter[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] += 1\n",
    "\n",
    "    # fetch attended region and average accumulated attention\n",
    "    zero_mask = counter == 0\n",
    "    overlay[~zero_mask] = overlay[~zero_mask] / counter[~zero_mask]\n",
    "    del counter\n",
    "\n",
    "    img = np.array(wsi.read_region((0, 0), vis_level, region_size).convert(\"RGB\"))\n",
    "\n",
    "    print('\\ncomputing heatmap image')\n",
    "    print('total of {} patches'.format(len(coords)))\n",
    "    twenty_percent_chunk = max(1, int(len(coords) * 0.2))\n",
    "\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "        norm = plt.Normalize(scores.min(), scores.max())\n",
    "\n",
    "    for idx in range(len(coords)):\n",
    "        if (idx + 1) % twenty_percent_chunk == 0:\n",
    "            print('progress: {}/{}'.format(idx, len(coords)))\n",
    "\n",
    "        score = scores[idx].item()\n",
    "        coord = coords[idx]\n",
    "\n",
    "        # attention block\n",
    "        raw_block = overlay[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]]\n",
    "\n",
    "        # image block (either blank canvas or orig image)\n",
    "        img_block = img[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]].copy()\n",
    "\n",
    "        # color block (cmap applied to attention block)\n",
    "        color_block = (cmap(norm(raw_block)) * 255)[:,:,:3].astype(np.uint8)\n",
    "\n",
    "        # copy over entire color block\n",
    "        img_block = color_block\n",
    "\n",
    "        # rewrite image block\n",
    "        img[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] = img_block.copy()\n",
    "\n",
    "    #return Image.fromarray(img) #overlay\n",
    "    print('Done')\n",
    "    del overlay\n",
    "\n",
    "    img = Image.fromarray(img)\n",
    "    w, h = img.size\n",
    "\n",
    "    if custom_downsample > 1:\n",
    "        img = img.resize((int(w/custom_downsample), int(h/custom_downsample)))\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "20b5df48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "creating heatmap for: \n",
      "w: 33333, h: 15653\n",
      "scaled patch size:  [256 256]\n",
      "\n",
      "computing heatmap image\n",
      "total of 3479 patches\n",
      "progress: 694/3479\n",
      "progress: 1389/3479\n",
      "progress: 2084/3479\n",
      "progress: 2779/3479\n",
      "progress: 3474/3479\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "heatmap = draw_heatmap(\n",
    "  scores=attention,\n",
    "  coords=coords,\n",
    "  wsi=wsi,\n",
    "  use_holes=True,\n",
    "  vis_level=1,\n",
    "  blank_canvas=False,\n",
    "  convert_to_percentiles=False\n",
    ")\n",
    "\n",
    "heatmap.save(os.path.join('data', 'interpretability', 'attention_heatmap.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8BKWfDSQwoQ_",
   "metadata": {
    "id": "8BKWfDSQwoQ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices to keep: [1722 2369 2696 2355 2692   76  263 2415  716 2416]\n",
      "Scores: [-2.2142136 -2.1994655 -2.184099  -2.1802504 -2.1670122 -2.1253953\n",
      " -1.9168873 -1.881992  -1.8797702 -1.75094  ]\n"
     ]
    }
   ],
   "source": [
    "# Extract the most important patches \n",
    "n_samples = 10 \n",
    "\n",
    "# sort attention weights \n",
    "to_keep = np.argsort(attention)[-n_samples:]\n",
    "scores_to_keep = attention[to_keep]\n",
    "coords_to_keep = coords[to_keep]\n",
    "\n",
    "print('Indices to keep:', to_keep)\n",
    "print('Scores:', scores_to_keep)\n",
    "\n",
    "for idx in range(n_samples):\n",
    "    patch = wsi.read_region(coords_to_keep[idx], 1, (256, 256))\n",
    "    patch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c32f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88af12fa",
   "metadata": {
    "id": "88af12fa"
   },
   "source": [
    "#### Interpretating with Integrated Gradients\n",
    "\n",
    "In this section, we will use Captum, an open-source package that provides off-the-shelf post-hoc interpretability techniques, including Integrated Gradients (IG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "hImkjH_VZB9h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hImkjH_VZB9h",
    "outputId": "6f5e8bbe-3e21-4f82-988c-c31e5a4224e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 543 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from captum) (3.5.1)\n",
      "Requirement already satisfied: numpy in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from captum) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from captum) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6->captum) (4.1.1)\n",
      "Requirement already satisfied: jinja2 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6->captum) (2.11.3)\n",
      "Requirement already satisfied: filelock in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6->captum) (3.6.0)\n",
      "Requirement already satisfied: sympy in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6->captum) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6->captum) (2.8.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.6->captum) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->captum) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/guillaumejaume/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.6->captum) (1.2.1)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593facd1",
   "metadata": {
    "id": "593facd1"
   },
   "source": [
    "1. **Notation:**\n",
    "   - Let's consider a deep learning model with an input vector `x` and output `f(x)`.\n",
    "   - The baseline or reference point is denoted as `x'`, typically chosen as a point with low complexity (e.g., all zeros or random noise).\n",
    "   - The attribution score for each input feature `i` is denoted as `A_i`.\n",
    "\n",
    "2. **Gradient Calculation:**\n",
    "   - Compute the gradients of the model's output with respect to the input features:\n",
    "   \n",
    "     $$\\vec{\\nabla} f(x) = \\left(\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \\ldots, \\frac{\\partial f(x)}{\\partial x_n}\\right)$$\n",
    "\n",
    "3. **Integrated Gradients Formula:**\n",
    "   - The Integrated Gradients score for each feature `i` is calculated as follows:\n",
    "   \n",
    "     $$A_i = (x_i - x'_i) \\times \\int_{\\alpha=0}^1 \\left(\\frac{\\partial f(x'+\\alpha(x-x'))}{\\partial x_i}\\right) d\\alpha$$\n",
    "\n",
    "4. **Explanation:**\n",
    "   - Integrated Gradients computes the contribution of each feature `i` by taking into account the difference between the input `x` and the baseline `x'`.\n",
    "   - It then integrates the gradients of the model's output with respect to feature `i` along a straight path from the baseline `x'` to the input `x`.\n",
    "   - The integral is calculated over a series of steps (α) from 0 to 1, representing the interpolation between the baseline and the input.\n",
    "   - The gradients at each interpolation point measure the sensitivity of the output to changes in feature `i` as we move from the baseline to the input.\n",
    "   - The contribution of feature `i` is multiplied by the difference between the input and baseline for that feature, capturing the change in the model's output caused by that feature.\n",
    "\n",
    "5. **Implementation Steps:**\n",
    "   - Select a baseline point `x'` (all zeros, random noise, or other relevant choice).\n",
    "   - Define the number of steps or intervals for the integration.\n",
    "   - For each step α from 0 to 1, calculate `x'+α(x-x')` as the intermediate input.\n",
    "   - Compute the gradients of the model's output with respect to the intermediate input at each step.\n",
    "   - Accumulate the gradients and multiply by the difference between `x` and `x'` for each feature `i` to calculate the attribution scores `A_i`.\n",
    "\n",
    "The Integrated Gradients technique provides feature-level attribution scores, allowing us to understand the importance of each input feature in the model's predictions. By visualizing or analyzing these scores, we can gain insights into which features are influential or critical for the model's decision-making process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "700f5efc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "700f5efc",
    "outputId": "54cb8bcd-27d6-4202-9a58-482135ac5f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3479])\n"
     ]
    }
   ],
   "source": [
    "# use captum to get IG scores.\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "def interpret_sample(features):\n",
    "    return model.captum(X=features)\n",
    "\n",
    "ig = IntegratedGradients(interpret_sample)\n",
    "features.requires_grad_()\n",
    "patch_preds = []\n",
    "for target in range(num_classes):\n",
    "    ig_attr = ig.attribute((features), n_steps=50, target=target)\n",
    "    ig_attr = ig_attr.squeeze().sum(dim=1).cpu().detach()\n",
    "    patch_preds.append(ig_attr)\n",
    "patch_preds = torch.stack(patch_preds, dim=0)\n",
    "\n",
    "print(patch_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aff39a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the histogram of IG scores\n",
    "scores = patch_preds[0, :].detach().numpy()\n",
    "bins = np.linspace(scores.min(), scores.max(), 50)\n",
    "plt.hist(scores, bins, histtype='bar', rwidth=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "YHQLG265dbLy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHQLG265dbLy",
    "outputId": "8b029e9b-a215-46f8-ed02-7d7e6205ba7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "creating heatmap for: \n",
      "w: 33333, h: 15653\n",
      "scaled patch size:  [256 256]\n",
      "\n",
      "computing heatmap image\n",
      "total of 3479 patches\n",
      "progress: 694/3479\n",
      "progress: 1389/3479\n",
      "progress: 2084/3479\n",
      "progress: 2779/3479\n",
      "progress: 3474/3479\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Heatmap with Captum\n",
    "heatmap = draw_heatmap(\n",
    "  scores=patch_preds[0, :],\n",
    "  coords=coords,\n",
    "  wsi=wsi,\n",
    "  cmap='jet',\n",
    "  alpha=1.0,\n",
    "  use_holes=True,\n",
    "  vis_level=1,\n",
    "  blank_canvas=False,\n",
    "  convert_to_percentiles=False\n",
    ")\n",
    "\n",
    "heatmap.save(os.path.join('data', 'interpretability', 'ig_heatmap.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5OWwQUiVdyqt",
   "metadata": {
    "id": "5OWwQUiVdyqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices to keep: tensor([ 360,  483, 1082, 2339,  295,  553,  581, 2273,  656, 2696])\n",
      "Scores: [0.00083727 0.00111626 0.00082688 0.00066204 0.00069693 0.00067952\n",
      " 0.00073171 0.00056845 0.00104312 0.00124933]\n"
     ]
    }
   ],
   "source": [
    "# Extract the most important patches \n",
    "n_samples = 10 \n",
    "\n",
    "# sort attention weights \n",
    "to_keep = np.argsort(patch_preds[0, :])[-n_samples:]\n",
    "scores_to_keep = attention[to_keep]\n",
    "coords_to_keep = coords[to_keep]\n",
    "\n",
    "print('Indices to keep:', to_keep)\n",
    "print('Scores:', scores_to_keep)\n",
    "\n",
    "for idx in range(n_samples):\n",
    "    patch = wsi.read_region(coords_to_keep[idx], 1, (256, 256))\n",
    "    patch.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375afc45",
   "metadata": {},
   "source": [
    "The following link at [http://clam.mahmoodlab.org](http://clam.mahmoodlab.org) visualizes high-attention heatmaps for LUAD vs LUSC subtyping via CLAM (similar to `ABMIL`) and confidence scores for each slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12b317",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "- What is the main advantage of IG over attention for model interpretability?\n",
    "\n",
    "\n",
    "- What are the limitations of feature-attribution methods?\n",
    "\n",
    "\n",
    "- What's the difference interpretability and explaianability? Link it to the notion of control.\n",
    "\n",
    "\n",
    "- If you were a clinical pathologist looking at these visualizations, what insights or concerns would you have in letting an AI algorithm assist you medical diagnoses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba3bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
