{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cc5a61",
   "metadata": {
    "id": "f4cc5a61"
   },
   "source": [
    "# Weakly-Supervised Deep Learning for Cancer Diagnosis in Computational Pathology\n",
    "\n",
    "- Presenter:\n",
    "   - Guillaume Jaume (gjaume@bwh.harvard.edu)\n",
    "   - Postdoctoral Researcher at Harvard Medical School and Brigham and Women's Hospital\n",
    "- Initally proposed and written by Richard J. Chen (richardchen@g.harvard.edu)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/10300839/232984533-c22822b8-df80-4b95-80e2-93dde2409bbf.png)\n",
    "\n",
    "**Definitions:**\n",
    "\n",
    "- *Computational pathology (CPath):* Computational methods based on the microscopic analysis of cells and tissues for the study of disease.\n",
    "\n",
    "- *Digital pathology:* A set of tools and systems for the acquisition, management and diagnosis of pathology glass slides in a digital setting.\n",
    "\n",
    "- *Whole slide image (WSI):* An image obtained by digitizing a glass slide at high-resolution using a scanner.\n",
    "\n",
    "- *Hematoxylin and eosin (H&E) staining:* The reference stain for histological analysis of tissues for visualization of cell nuclei (in purple) with extracellular information and cytoplasm (in pink).\n",
    "\n",
    "\n",
    "**Background**:\n",
    "\n",
    "Computational Pathology aims to automate, assist, and augment the clinical practice of pathology using computational tools based on Artificial Intelligence.\n",
    "\n",
    "Tissue phenotyping is a fundamental problem in computational pathology (CPATH) to characterize histopathologic features for cancer diagnosis, prognosis, and prediction of treatment response. Unlike natural images, whole-slide imaging is a challenging computer vision domain in which image resolutions can be as large as $150{,}000 \\times 150{,}000$ pixels (>50 GB to load the entire image in RAM).\n",
    "\n",
    "To address this computational and memory bottleneck, the majority of state-of-the-art methods use a three-stage, weakly-supervised pipeline based on multiple instance learning (MIL):\n",
    "1. Tissue patching at a single magnification objective (\"zoom\"), e.g., 20x magnification\n",
    "\n",
    "2. Patch-level feature extraction to construct a set of patch embeddings (compress patches by a factor 100~500)\n",
    "\n",
    "3. Global pooling of embeddings to construct a slide-level representation for weak-supervision using slide-level labels (e.g., subtype, grade, stage, survival, origin).\n",
    "\n",
    "**Notebook Objective**: The following tutorial aims to distinguish Lung Adenocarcinoma (LUAD, 40% of all lung cancer) vs. Lung Squamous Cell Carcinoma (LUSC, 30% of all lung cancer) (see [Lu et al., Data-efficient and weakly supervised computational pathology on whole-slide images  Nature BME 2021](https://www.nature.com/articles/s41551-020-00682-w) and codebase [CLAM](https://github.com/mahmoodlab/CLAM). Specifically, we will:\n",
    "- Train and evaluate a \"naive\" MIL algorithm called `AverageMIL`, which takes the average of patch embeddings (as the global pooling operator). \n",
    "\n",
    "- Implement a more sophisticated algorithm called Attention-Based Multiple Instance Learning (`ABMIL`), which learns attention weights for computing a weighted average of patch embeddings.\n",
    "\n",
    "- Compare and contrast `AverageMIL` and `ABMIL`, discussing which algorithm performs better and potential limitations.\n",
    "\n",
    "**About this notebook**:\n",
    "- Model implementation and training is directly adapted from [CLAM](https://github.com/mahmoodlab/CLAM). CLAM includes many additional features (e.g. - letting users set up optimizers, model types, logging information, and other hyper-parameters) left out due to making this notebook as simple to run as possible for teaching purposes. To use all features, please see CLAM.\n",
    "\n",
    "- Though this notebook is based off of CLAM, the method-of-interest that you will be implementing is not CLAM, but a different method called ABMIL from [Ilse et al. Attention-Based Multiple Instance Learning ICML 2018](https://arxiv.org/abs/1802.04712), which CLAM is derived from.\n",
    "\n",
    "- Though pre-extracted features were generated using the CLAM codebase, the encoder was not a truncated ResNet-50 pretrained on ImageNet (dimension 1024) at 20 $\\times$ resolution. Instead, we extracted features with a much smaller CNN encoder (dimension 320) at 10 $\\times$ resolution, which shrinks the size of the dataset from ~11 GB to ~3.96 GB of storage (**download link for pre-extracted features in the cell below**). In addition, a torch.seed is set for reproducibility (all outputs should be deterministic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ed950",
   "metadata": {
    "id": "8f7ed950"
   },
   "source": [
    "### Colab Installation, Data Download, & Dependencies\n",
    "\n",
    "- Gets clinical metadata csv for tcga-luad and tcga-lusc with predefined train/val/test splits\n",
    "- Gets pre-extracted features for tcga-luad and tcga-lusc diagnostic WSIs (1043 WSIs total, ~3.96 GB in size, ~67 seconds to download)\n",
    "\n",
    "Alternatively, you can download the data directly from Dropbox to your local computer, and run this Colab Notebook locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QKGr936RiVFS",
   "metadata": {
    "id": "QKGr936RiVFS"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/5wuvu791vwntg9o/tcga_lung_splits.csv\n",
    "!wget https://www.dropbox.com/s/euepd2owxvuwr7v/feats_pt.zip\n",
    "!unzip -q feats_pt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a99a0",
   "metadata": {
    "id": "8c5a99a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53517df",
   "metadata": {
    "id": "e53517df"
   },
   "source": [
    "### WSI data preprocessing for histology slides in the TCGA-Lung cohort\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/10300839/232984010-7f8e3a6f-e0c5-4847-8d0f-747460055528.png)\n",
    "\n",
    "To process WSIs, tools such as [CLAM](https://github.com/mahmoodlab/CLAM) are typically used for tissue patching and non-overlapping patch feature extraction. Though easy-to-use, using CLAM for feature processing would require downloading gigapixel WSIs (> 1000 WSIS in TCGA-LUAD and TCGA-LUSC), which exceeds over >100GB of storage space. To alleviate this issue, this problem set provides pre-extracted features (processed via CLAM, but using a much smaller vision encoder with $D = 320$). However, to still illustrate how CLAM preprocessing works, the cell below describes a high-level overview on how WSIs are formulated as `[M x D]`-dim bag of patch embeddings, where `M` is the number of tissue patches and `D` is the hidden dimension size of your encoder. Again, please use [CLAM](https://github.com/mahmoodlab/CLAM) if you are interested in re-generating these features.\n",
    "\n",
    "**Note:** This cell doesn't need to be run to train the final models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a7d83",
   "metadata": {
    "id": "a94a7d83"
   },
   "outputs": [],
   "source": [
    "# Let's say we have a \"set of M [256 x 256 x 3] image patches (M = 512), which are taken from non-overlapping patches in the WSI.\n",
    "M = 2\n",
    "X = torch.randn(M, 3, 256, 256) # Arranged in (Batch, Channel, Width, Height) format or (B, C, W, H) for short\n",
    "print(\"WSI Shape:\", X.shape)\n",
    "\n",
    "# We would for instance use a CNN model (pretrained on ImageNet) as our vision encoder for pre-extracting \"compressed\" representations from each patch\n",
    "cnn = torchvision.models.mobilenet_v3_small()\n",
    "cnn.eval()\n",
    "\n",
    "# Since this model was taken from torchvision and trained on ImageNet, the output of the model are the probability scores of the ImageNet classes (1000 classes total).\n",
    "# To extract useful features from each patch, we have to use the penultimate layer(s) of the CNN, before feeding it into a linear layer.\n",
    "print(\"Probability Scores for ImageNet:\", cnn.forward(X[:1]).shape)\n",
    "\n",
    "# To extract the penultimate features, we can define a new function that returns the features\n",
    "# before giving it to internal classifier layer within the model.\n",
    "# Again, we want to use the pretrained features on ImageNet, but don't want the classification scores for \"ImageNet\" classes!\n",
    "# # See the below documentation for how the forward pass in MobileNetV3 works.\n",
    "# https://pytorch.org/vision/main/_modules/torchvision/models/mobilenetv3.html#mobilenet_v3_small\n",
    "encoder = lambda x: torch.flatten(cnn.avgpool(cnn.features(x)), 1)\n",
    "print(\"Feature Embedding Shape:\", encoder(X[:1]).shape)\n",
    "\n",
    "# We can now use our encoder to extract features for each patch.\n",
    "# Typically, the # of non-overlapping patches in a WSI is ~15,000. Thus, we often have to extract patch features in mini-batches.\n",
    "batch_size = 32\n",
    "H = []\n",
    "for bag_idx in range(0, M, batch_size):\n",
    "    H.append(encoder(X[bag_idx:(bag_idx+batch_size)]).cpu().detach().numpy())\n",
    "print(\"Bag Shape\", np.vstack(H).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ohfe2sWOUD5q",
   "metadata": {
    "id": "Ohfe2sWOUD5q"
   },
   "source": [
    "### Data Exploration\n",
    "\n",
    "**Note:** This cell doesn't need to be run to train the final models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R1rXRyqPRaKk",
   "metadata": {
    "id": "R1rXRyqPRaKk"
   },
   "outputs": [],
   "source": [
    "# where we downloaded the features and label csv to\n",
    "feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
    "\n",
    "# label csv matches case_id (patient), slide_id (WSI image filename), and diagnosis (LUAD vs LUSC)\n",
    "# as well as pre-defined splits (train / val / test)\n",
    "df = pd.read_csv(csv_fpath)\n",
    "display(df)\n",
    "display(df[['split', 'OncoTreeCode']].value_counts())\n",
    "\n",
    "# extracted feature filenames + slide_id column match\n",
    "feats_pt_fnames = pd.Series(os.listdir(feats_dirpath))\n",
    "print(\"Example filenames for extracted features:\", list(feats_pt_fnames[:5]))\n",
    "print(\"Overlap of extracted feature filenames + slide_id column:\",\n",
    "      len(set(df['slide_id']).intersection(set(feats_pt_fnames.str[:-3]))))\n",
    "\n",
    "# statistics about the size of each bag\n",
    "bag_sizes = []\n",
    "for e in os.scandir(feats_dirpath):\n",
    "    feats_pt = torch.load(e.path)    # [M x d]-dim tensor\n",
    "    bag_sizes.append(feats_pt.shape[0])\n",
    "print('Mean Bag Size:', np.mean(bag_sizes))\n",
    "print('Std Bag Size:', np.std(bag_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cddf94",
   "metadata": {
    "id": "50cddf94"
   },
   "source": [
    "### Model 1:  AverageMIL \n",
    "\n",
    "Implemented is a minimalistic training setup that performs weakly-supervised learning via `AverageMIL` on LUAD vs. LUSC subtyping using 1043 diagnostic H\\&E tissue slides from the The Cancer Genome Atlas (features already pre-extracted and downloaded from installation, clinical metadata for all case and slide IDs also downloaded).\n",
    "\n",
    "You can run the cells in the Google Colab Notebook and see how well this algorithm performs in 20 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a3278",
   "metadata": {
    "id": "bd9a3278"
   },
   "outputs": [],
   "source": [
    "class AverageMIL(nn.Module):\n",
    "    def __init__(self, input_dim=320, hidden_dim=64, dropout=0.25, n_classes=2):\n",
    "        r\"\"\"\n",
    "        AverageMIL, a naive MIL algorithm that average pools all patch features.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): input feature dimension.\n",
    "            hidden_dim (int): hidden layer dimension.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        super(AverageMIL, self).__init__()\n",
    "        self.inst_level_fc = nn.Sequential(*[nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]) # Fully-Connected Layer, applied \"instance-wise\" to each embedding\n",
    "        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)                                            # Bag-Level Classifier\n",
    "\n",
    "    def forward(self, H):\n",
    "        r\"\"\"\n",
    "        Takes as input a [M x D]-dim bag of patch features (representing a WSI), and outputs: 1) logits for classification, 2) un-normalized attention scores.\n",
    "\n",
    "        Args:\n",
    "            H (torch.Tensor): [M x D]-dim bag of patch features (representing a WSI)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [1 x n_classes]-dim tensor of un-normalized logits for classification task.\n",
    "            None (no attention scores to return)\n",
    "        \"\"\"\n",
    "        H = self.inst_level_fc(H)                   # 1. Preprocesses each \"instance-level\" embedding to be \"hidden-dim\"-dim size\n",
    "        z = H.mean(dim=0).unsqueeze(dim=0)          # 2. Average of Patch Embeddings\n",
    "        logits = self.bag_level_classifier(z)       # 3. Bag-Level Classifier\n",
    "        return logits, None\n",
    "\n",
    "\n",
    "class MILDataset(torch.utils.data.dataset.Dataset):\n",
    "    r\"\"\"\n",
    "    torch.utils.data.dataset.Dataset object that loads pre-extracted features per WSI from a CSV.\n",
    "\n",
    "    Args:\n",
    "        feats_dirpath (str): Path to pre-extracted patch features (assumes that these features are saved as a *.pt object with it's corresponding slide_id as the filename)\n",
    "        csv_fpath (str): Path to CSV file which contains: 1) Case ID, 2) Slide ID, 3) split information (train / val / test), and 4) label columns of interest for classification.\n",
    "        which_split (str): Split that is used for subsetting the CSV (choices: ['train', 'val', 'test'])\n",
    "        n_classes (int): Number of classes (default == 2 for LUAD vs LUSC subtyping)\n",
    "    \"\"\"\n",
    "    def __init__(self, feats_dirpath='./', csv_fpath='./tcga_lung_splits.csv', which_split='train', which_labelcol='OncoTreeCode_Binarized'):\n",
    "        self.feats_dirpath, self.csv, self.which_labelcol = feats_dirpath, pd.read_csv(csv_fpath), which_labelcol\n",
    "        self.csv_split = self.csv[self.csv['split']==which_split]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = torch.load(os.path.join(self.feats_dirpath, self.csv_split.iloc[index]['slide_id']+'.pt'))\n",
    "        label = self.csv_split.iloc[index][self.which_labelcol]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv_split.shape[0]\n",
    "\n",
    "\n",
    "def traineval_epoch(epoch, model, loader, optimizer=None, loss_fn=nn.CrossEntropyLoss(), split='train', device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), verbose=1, print_every=300):\n",
    "    r\"\"\"\n",
    "    Function that performs one epoch of training / evaluation with torch.nn model over torch.utils.data.DataLoader object.\n",
    "    Typically, these functions are defined separately for training and validation, but to save line space, we have combined the two.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current epoch of training / evaluation (used for logging).\n",
    "        model (torch.nn): MIL model for processing bag of patch features.\n",
    "        loader (torch.utils.data.DataLoader): Object for getting bag of patch features per WSI.\n",
    "        loss_fn (torch.nn): Loss function.\n",
    "        split (str): Which split, used for setting up model + calculating loss + calculating gradients.\n",
    "        device (torch): Object representing the device on which a torch.Tensor will be allocated.\n",
    "        verbose (int): Whether to print summary epoch results (verbose >=1) and iteration info (verbose >=2).\n",
    "        print_every (int): How many batch iterations\n",
    "\n",
    "    Returns:\n",
    "        log_dict (dict): Dictionary for logging loss and performance for train / val / test split.\n",
    "    \"\"\"\n",
    "    model.train() if (split == 'train') else model.eval()       # turning on whether model should be used for training or evaluation\n",
    "    total_loss, Y_probs, labels = 0.0, [], []                   # tracking loss + logits/labels for performance metrics\n",
    "    for batch_idx, (X_bag, label) in enumerate(loader):\n",
    "        # Since we assume batch size == 1, we want to prevent torch from collating our bag of patch features as [1 x M x D] torch tensors.\n",
    "        X_bag, label = X_bag[0].to(device), label.to(device)\n",
    "\n",
    "        if (split == 'train'):\n",
    "            logits, A_norm = model(X_bag)\n",
    "            loss = loss_fn(logits, label)\n",
    "            loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "        else:\n",
    "            with torch.no_grad(): logits, A_norm = model(X_bag)\n",
    "            loss = loss_fn(logits, label)\n",
    "\n",
    "        # Track total loss, logits, and current progress\n",
    "        total_loss += loss.item()\n",
    "        Y_probs.append(torch.softmax(logits, dim=-1).cpu().detach().numpy())\n",
    "        labels.append(label.cpu().detach().numpy())\n",
    "        if ((batch_idx + 1) % print_every == 0) and (verbose >= 2):\n",
    "            print(f'Epoch {epoch}:\\t Batch {batch_idx}\\t Avg Loss: {total_loss / (batch_idx+1):.04f}\\t Label: {label.item()}\\t Bag Size: {X_bag.shape[0]}')\n",
    "\n",
    "    # Compute balanced accuracy and AUC-ROC from saved logits / labels\n",
    "    Y_probs, labels = np.vstack(Y_probs), np.concatenate(labels)\n",
    "    log_dict = {f'{split} loss': total_loss/len(loader),\n",
    "                f'{split} acc': sklearn.metrics.balanced_accuracy_score(labels, Y_probs.argmax(axis=1)),\n",
    "                f'{split} auc': sklearn.metrics.roc_auc_score(labels, Y_probs[:, 1])}\n",
    "\n",
    "    # Print out end-of-epoch information\n",
    "    if (verbose >= 1):\n",
    "        print(f'### ({split.capitalize()} Summary) ###')\n",
    "        print(f'Epoch {epoch}:\\t' + f'\\t'.join([f'{k.capitalize().rjust(10)}: {log_dict[k]:.04f}' for k,v in log_dict.items()]))\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e79d24",
   "metadata": {
    "id": "71e79d24"
   },
   "outputs": [],
   "source": [
    "# Sets the random seed (for reproducibility)\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# Get data loaders for train-val-test split evaluation\n",
    "feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
    "loader_kwargs = {\n",
    "    'batch_size': 1,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': False\n",
    "} \n",
    "train_dataset, val_dataset, test_dataset = [MILDataset(feats_dirpath, csv_fpath, which_split=split) for split in ['train', 'val', 'test']]\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# Get model, optimizer, and loss function\n",
    "device = torch.device('cpu')\n",
    "model = AverageMIL(input_dim=320, hidden_dim=64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set-up train-validation loop and early stopping\n",
    "num_epochs, min_early_stopping, patience, counter = 20, 10, 5, 0\n",
    "lowest_val_loss, best_model = np.inf, None\n",
    "all_train_logs, all_val_logs = [], [] \n",
    "for epoch in range(num_epochs):\n",
    "    train_log = traineval_epoch(epoch, model, train_loader, optimizer=optimizer, split='train', device=device, verbose=2, print_every=200)\n",
    "    val_log = traineval_epoch(epoch, model, val_loader, optimizer=None, split='val', device=device, verbose=1)\n",
    "    val_loss = val_log['val_loss']\n",
    "    # Early stopping: If validation loss does not go down for <patience> epochs after <min_early_stopping> epochs, stop model training early\n",
    "    if (epoch > min_early_stopping):\n",
    "        if (val_loss < lowest_val_loss):\n",
    "            print(f'Resetting early-stopping counter: {lowest_val_loss:.04f} -> {val_loss:.04f}...')\n",
    "            lowest_val_loss, counter, best_model = val_loss, 0, copy.deepcopy(model)\n",
    "        else:\n",
    "            print(f'Early-stopping counter updating: {counter}/{patience} -> {counter+1}/{patience}...')\n",
    "            counter += 1\n",
    "\n",
    "    if counter >= patience: break\n",
    "    print()\n",
    "\n",
    "# Report best model (lowest validation loss) on test split\n",
    "best_model = model if (best_model is None) else best_model\n",
    "test_log = traineval_epoch(epoch, best_model, test_loader, optimizer=None, split='test', device=device, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad43256",
   "metadata": {
    "id": "4ad43256"
   },
   "source": [
    "### Model 2. Implement Attention-Based Multiple Instance Learning (ABMIL) \n",
    "\n",
    "Following your experimentation with `AverageMIL`, you are ready to implement a more sophisticated model for LUAD vs. LUSC subtyping. Formally, let $\\mathbf{H}=\\left\\{\\mathbf{h}_1, \\ldots, \\mathbf{h}_M\\right\\} \\in \\mathbb{R}^{M \\times D}$  be a bag of $M$ patch embeddings, with each embedding having dimension size $D$. Ilse et al. 2018 proposed the following attention-based MIL pooling operation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} =\\sum_{i=1}^M a_i \\mathbf{h}_i, \\quad \\text{where} \\enspace a_i=\\frac{\\exp \\left\\{\\mathbf{w}^{\\top}\\left(\\tanh \\left(\\mathbf{V h}_{i} ^ { \\top }\\right) \\odot \\operatorname{sigm}\\left(\\mathbf{U h}_i^{\\top}\\right)\\right)\\right\\}}{\\sum_{j=1}^M \\exp \\left\\{\\mathbf{w}^{\\top}\\left(\\tanh \\left(\\mathbf{V} \\mathbf{h}_j^{\\top}\\right) \\odot \\operatorname{sigm}\\left(\\mathbf{U h}_j^{\\top}\\right)\\right)\\right\\}}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^{L \\times 1}$, $\\mathbf{V} \\in \\mathbb{R}^{L \\times D}$, and $\\mathbf{U} \\in \\mathbb{R}^{L \\times D}$ are learnable neural network parameters (implemented as fully-connected layers), and $\\mathbf{z} \\in \\mathbb{R}^{D}$ is the weighted average of all patch embeddings in $\\mathbf{H}$. The hyperbolic tangent $\\tanh (\\cdot)$ element-wise non-linearity and sigmoid non-linearity are utilized for proper gradient flow.\n",
    "\n",
    "Via PyTorch, the mathematical expression for computing $a_m$ is implemented as the `torch.nn` module `AttentionTanhSigmoidGating`, which we use as a layer in `ABMIL` for calculating the weighted average of patch embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da5f05",
   "metadata": {
    "id": "09da5f05"
   },
   "outputs": [],
   "source": [
    "class AttentionTanhSigmoidGating(nn.Module):\n",
    "    def __init__(self, D=64, L=64, dropout=0.25):\n",
    "        r\"\"\"\n",
    "        Global attention pooling layer with tanh non-linearity and sigmoid gating (Ilse et al. 2018).\n",
    "\n",
    "        Args:\n",
    "            D (int): input feature dimension.\n",
    "            L (int): hidden layer dimension. Notation changed from M from Ilse et al 2018, as M is overloaded to also describe # of patch embeddings in a WSI.\n",
    "            dropout (float): Dropout probability.\n",
    "\n",
    "        Returns:\n",
    "            A_norm (torch.Tensor): [M x 1]-dim tensor of normalized attention scores (sum to 1)\n",
    "        \"\"\"\n",
    "        super(Attention_TanhSigmoidGating, self).__init__()\n",
    "        self.tanhV = nn.Sequential(*[nn.Linear(D, L), nn.Tanh(), nn.Dropout(dropout)])\n",
    "        self.sigmU = nn.Sequential(*[nn.Linear(D, L), nn.Sigmoid(), nn.Dropout(dropout)])\n",
    "        self.w = nn.Linear(L, 1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        A_raw = self.w(self.tanhV(H).mul(self.sigmU(H))) # exponent term\n",
    "        A_norm = F.softmax(A_raw, dim=0)                 # apply softmax to normalize weights to 1\n",
    "        assert abs(A_norm.sum() - 1) < 1e-3              # Assert statement to check sum(A) ~= 1\n",
    "        return A_norm\n",
    "\n",
    "\n",
    "class ABMIL(nn.Module):\n",
    "    def __init__(self, input_dim=320, hidden_dim=64, dropout=0.25, n_classes=2):\n",
    "        r\"\"\"\n",
    "        Attention-Based Multiple Instance Learning (Ilse et al. 2018).\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): input feature dimension.\n",
    "            hidden_dim (int): hidden layer dimension.\n",
    "            dropout (float): Dropout probability.\n",
    "            n_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        super(ABMIL, self).__init__()\n",
    "        self.inst_level_fc = nn.Sequential(*[nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]) # Fully-Connected Layer, applied \"instance-wise\" to each embedding\n",
    "        self.global_attn = AttentionTanhSigmoidGating(L=hidden_dim, D=hidden_dim)                              # Attention Function\n",
    "        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)                                            # Bag-Level Classifier\n",
    "\n",
    "    def forward(self, X: torch.randn(100, 320)):\n",
    "        r\"\"\"\n",
    "        Takes as input a [M x D]-dim bag of patch features (representing a WSI), and outputs: 1) logits for classification, 2) un-normalized attention scores.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): [M x D]-dim bag of patch features (representing a WSI)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [1 x n_classes]-dim tensor of un-normalized logits for classification task.\n",
    "            A_norm (torch.Tensor): [M,]- or [M x 1]-dim tensor of attention scores.\n",
    "        \"\"\"\n",
    "        H_inst = self.inst_level_fc(X)         # 1. Process each feature embedding to be of size \"hidden-dim\"\n",
    "        A_norm = self.global_attn(H_inst)      # 2. Get normalized attention scores for each embedding (s.t. sum(A_norm) ~= 1)\n",
    "        z = torch.sum(A_norm * H_inst, dim=0)  # 3. Output of global attention pooling over the bag\n",
    "        logits = self.bag_level_classifier(z)   # 4. Get un-normalized logits for classification task\n",
    "        try:\n",
    "            assert logits.shape == (1,2)\n",
    "        except:\n",
    "            print(f\"Logit tensor shape is not formatted correctly. Should output [1 x 2] shape, but got {logits.shape} shape\")\n",
    "        return logits, A_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703caf8b",
   "metadata": {
    "id": "703caf8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sets the random seed (for reproducibility)\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "# Get data loaders for train-val-test split evaluation\n",
    "feats_dirpath, csv_fpath = './feats_pt/', './tcga_lung_splits.csv'\n",
    "display(pd.read_csv(csv_fpath).head(10)) # visualize data\n",
    "loader_kwargs = {'batch_size': 1, 'num_workers': 2, 'pin_memory': False} # Batch size set to 1 due to variable bag sizes. Hard to collate.\n",
    "train_dataset, val_dataset, test_dataset = [MILDataset(feats_dirpath, csv_fpath, which_split=split) for split in ['train', 'val', 'test']]\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "# Get model, optimizer, and loss function\n",
    "device = torch.device('cpu')\n",
    "model = ABMIL(input_dim=320, hidden_dim=64).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set-up train-validation loop and early stopping\n",
    "num_epochs, min_early_stopping, patience, counter = 20, 10, 5, 0\n",
    "lowest_val_loss, best_model = np.inf, None\n",
    "all_train_logs, all_val_logs = [], [] # TODO: do something with train_log / val_log every epoch to help visualize performance curves?\n",
    "for epoch in range(num_epochs):\n",
    "    train_log = traineval_epoch(epoch, model, train_loader, optimizer=optimizer, split='train', device=device, verbose=2, print_every=200)\n",
    "    val_log = traineval_epoch(epoch, model, val_loader, optimizer=None, split='val', device=device, verbose=1)\n",
    "    val_loss = val_log['val loss']\n",
    "\n",
    "    # Early stopping: If validation loss does not go down for <patience> epochs after <min_early_stopping> epochs, stop model training early\n",
    "    if (epoch > min_early_stopping):\n",
    "        if (val_loss < lowest_val_loss):\n",
    "            print(f'Resetting early-stopping counter: {lowest_val_loss:.04f} -> {val_loss:.04f}...')\n",
    "            lowest_val_loss, counter, best_model = val_loss, 0, copy.deepcopy(model)\n",
    "        else:\n",
    "            print(f'Early-stopping counter updating: {counter}/{patience} -> {counter+1}/{patience}...')\n",
    "            counter += 1\n",
    "\n",
    "    if counter >= patience: break\n",
    "    print()\n",
    "\n",
    "# Report best model (lowest validation loss) on test split\n",
    "best_model = model if (best_model is None) else best_model\n",
    "test_log = traineval_epoch(epoch, best_model, test_loader, optimizer=None, split='test', device=device, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d450527",
   "metadata": {
    "id": "7d450527"
   },
   "source": [
    "### Discussion. Compare and Contrast AverageMIL and ABMIL\n",
    "\n",
    "Compare and contrast the **validation** and **test** performance of `AverageMIL` and `ABMIL`. In particular:\n",
    "\n",
    "2. Which model performed better on overall AUC and balanced accuracy on the **test split**? Which class (LUAD or LUSC) was more prone to mis-classification by each model?\n",
    "3. The following link at [http://clam.mahmoodlab.org](http://clam.mahmoodlab.org) visualizes high-attention heatmaps for LUAD vs LUSC subtyping via CLAM (similar to `ABMIL`) and confidence scores for each slides. If you were a clinical pathologist looking at these visualizations, what insights or concerns would you have in letting an AI algorithm assist you medical diagnoses?\n",
    "4. The experimental setup in this problem set is limited to only evaluating on data from TCGA. List three techniques used in Lu et al. 2021 (or other relevant biomedical imaging $\\times$ AI studies) that could be used in assessing 1) data efficiency, 2) generalization performance, and 3) concordance of attention-based interpretability of `ABMIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689affba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab8319496eb09321cf5597fd6abbcdcc0a1b57046a5c3edfd9bd1380f7f3d177"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
